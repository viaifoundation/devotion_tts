# Dockerfile for Qwen3-TTS on NVIDIA DGX Spark (ARM64/Blackwell)
# Supports Qwen2.5-TTS 1.7B Model

FROM nvcr.io/nvidia/pytorch:25.11-py3

# Set working directory
WORKDIR /workspace/github/devotion_tts

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    libsndfile1 \
    pkg-config \
    cmake \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Install torchaudio from source (Required for NVIDIA container compatibility)
# Disabled CUDA build for torchaudio to align with system Torch
# Also remove torchvision to prevent conflicts (not needed for TTS)
RUN pip uninstall -y torchvision && \
    USE_CUDA=0 pip install git+https://github.com/pytorch/audio.git@v2.5.1 --no-deps --no-build-isolation

# Install Qwen3-TTS dependencies
# We uninstall transformers first to ensure we get a clean install if needed, 
# although we want to be careful not to break the container's optimized environment.
# Since Qwen2.5-TTS likely needs a recent transformers, checking version compatibility is key.
# The base image likely has transformers installed.

# Tier 1: Utilities
RUN pip install --no-cache-dir \
    numpy \
    scipy \
    soundfile \
    librosa \
    pydub \
    inflect \
    tqdm \
    loguru \
    "typing-extensions" \
    accelerate \
    einops \
    transformers_stream_generator \
    tiktoken \
    gradio \
    "fsspec<=2025.10.0"

# Tier 2: AI Core (Install with NO DEPS to prevent torch downgrade)
# Warning: Do NOT remove --no-deps or torch will be downgraded!
RUN pip install --no-cache-dir --no-deps \
    "transformers>=4.40.0" \
    "huggingface-hub"

# Install Qwen3-TTS package (Critical for model architecture support)
# Use --no-deps to ensure it doesn't pull standard torch
RUN pip install --no-cache-dir --no-deps git+https://github.com/QwenLM/Qwen3-TTS.git

# Attempt to install flash-attn. Steps taken to maximize success on ARM64:
# 1. Ensure ninja is installed (usually is in these containers)
# 2. Use --no-build-isolation to use the container's torch
RUN pip install ninja && \
    pip install flash-attn --no-build-isolation || echo "⚠️ Flash Attention build failed, continuing without it. Performance may be degraded."

# Default command
CMD ["/bin/bash"]
