# gen_prayer_vibe.py
# VibeVoice Workflow for Prayer (Microsoft/OpenSource)

import torch
import numpy as np
import re
import sys
import os
import warnings
from datetime import datetime
from pydub import AudioSegment

# Silence noisy libraries
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Setup path to find VibeVoice (sibling directory)
VIBEVOICE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "../VibeVoice"))

if os.path.exists(VIBEVOICE_PATH):
    sys.path.append(VIBEVOICE_PATH)
else:
    print(f"‚ö†Ô∏è Warning: VibeVoice path not found at {VIBEVOICE_PATH}")
    sys.exit(1)

try:
    from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference
    from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor
except ImportError as e:
    print(f"‚ùå Failed to import VibeVoice: {e}")
    sys.exit(1)

from bible_parser import convert_bible_reference
from date_parser import convert_dates_in_text
from text_cleaner import clean_text
import filename_parser
import audio_mixer

import argparse

# CLI Args
if "-?" in sys.argv:
    print(f"Usage: python {sys.argv[0]} [--prefix PREFIX] [--help]")
    print("Options:")
    print("  --prefix PREFIX      Filename prefix (overrides 'FilenamePrefix' in text)")
    print("  --help, -h           Show this help")
    sys.exit(0)

parser = argparse.ArgumentParser()
parser.add_argument("--prefix", type=str, default=None, help="Filename prefix")
args, unknown = parser.parse_known_args()
CLI_PREFIX = args.prefix

ENABLE_BGM = False
BGM_FILE = "AmazingGrace.MP3"

TEXT = """
‰∫≤Áà±ÁöÑÂ§©Áà∂Ôºö
Êàë‰ª¨ÊÑüË∞¢‰Ω†ÔºåÂõ†‰Ω†ÁöÑÊÅ©ÂÖ∏ÊØè‰∏ÄÂ§©ÈÉΩÊòØÊñ∞ÁöÑÔºÅ
Âú®Ëøô‰∏™ÂÆâÈùôÁöÑÊó∂ÂàªÔºåÊàë‰ª¨Â∞ÜÂøÉÂÖ®ÁÑ∂Âêë‰Ω†ÊïûÂºÄ„ÄÇÊ±Ç‰Ω†‰øùÂÆàÊàë‰ª¨ÁöÑÂøÉÊÄùÊÑèÂøµÔºåËÆ©Êàë‰ª¨Âú®ÂøôÁ¢åÁöÑÁîüÊ¥ª‰∏≠Ôºå‰æùÁÑ∂ËÉΩÂê¨ËßÅ‰Ω†ÂæÆÂ∞èÁöÑÂ£∞Èü≥„ÄÇ
‰∏ªÂïäÔºåÊ±Ç‰Ω†ËµêÁªôÊàë‰ª¨Â±ûÂ§©ÁöÑÊô∫ÊÖßÔºåËÆ©Êàë‰ª¨Âú®Èù¢ÂØπÊåëÊàòÊó∂Ôºå‰∏ç‰æùÈù†Ëá™Â∑±ÁöÑËÅ™ÊòéÔºåËÄåÊòØÂçïÂçï‰ª∞Êúõ‰Ω†„ÄÇ
ÊÑø‰Ω†ÁöÑÂπ≥ÂÆâÂÖÖÊª°Êàë‰ª¨ÁöÑÂÆ∂Â∫≠ÔºåÊÑø‰Ω†ÁöÑÁà±ÊµÅÊ∑åÂú®Êàë‰ª¨ÂΩºÊ≠§‰πãÈó¥„ÄÇ
‰πüÊ±Ç‰Ω†ËÆ∞ÂøµÈÇ£‰∫õÂú®ÁóÖÁóõÂíåËΩØÂº±‰∏≠ÁöÑËÇ¢‰ΩìÔºåÊÑø‰Ω†ÁöÑÂåªÊ≤ª‰∏¥Âà∞‰ªñ‰ª¨Ôºå‰Ωø‰ªñ‰ª¨ÈáçÊñ∞ÂæóÂäõ„ÄÇ
ÊÑüË∞¢ËµûÁæé‰∏ªÔºåÂê¨Êàë‰ª¨‰∏çÈÖçÁöÑÁ•∑ÂëäÔºåÂ•â‰∏ªËÄ∂Á®£Âü∫Áù£ÂæóËÉúÁöÑÂêçÊ±ÇÔºÅÈòøÈó®ÔºÅ
(ËÖìÁ´ãÊØî‰π¶ 4:6-7) 12/14/2025
"""


# Configuration
MODEL_PATH = "microsoft/VibeVoice-Realtime-0.5B"  # HuggingFace model hub path
VOICES_DIR = os.path.join(VIBEVOICE_PATH, "demo", "voices", "streaming_model")

# Helper Class for Voice Mapping (Adapted from demo)
class VoiceMapper:
    def __init__(self, voices_dir):
        self.voices_dir = voices_dir
        self.voice_presets = {}
        self.setup_voice_presets()

    def setup_voice_presets(self):
        if not os.path.exists(self.voices_dir):
            print(f"‚ö†Ô∏è Warning: Voices directory not found at {self.voices_dir}")
            return
        
        # Scan for .pt files (voice embeddings)
        for root, _, files in os.walk(self.voices_dir):
            for file in files:
                if file.endswith(".pt"):
                    name = os.path.splitext(file)[0]
                    path = os.path.join(root, file)
                    self.voice_presets[name] = path
                    
                    # Also map simplified names
                    # e.g. en-Mike_man -> en-Mike
                    if '_' in name:
                        name = name.split('_')[0]
                        self.voice_presets[name] = path
                    
                    # e.g. en-Mike -> Mike
                    if '-' in name:
                        short_name = name.split('-')[-1]
                        self.voice_presets[short_name] = path

    def get_voice_path(self, name):
        return self.voice_presets.get(name) or self.voice_presets.get("Mike") # Default to Mike

# Load Model
print(f"Loading VibeVoice model: {MODEL_PATH}...")
try:
    device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"Using device: {device}")

    # DataType and Attn Config
    if device == "cuda":
        torch_dtype = torch.bfloat16
        attn_impl = "flash_attention_2"
    elif device == "mps":
        torch_dtype = torch.float32
        attn_impl = "sdpa"
    else:
        torch_dtype = torch.float32
        attn_impl = "sdpa"

    processor = VibeVoiceStreamingProcessor.from_pretrained(MODEL_PATH)
    
    # Load model with fallback logic for attention
    try:
        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch_dtype,
            attn_implementation=attn_impl,
            device_map=device if device != "mps" else None
        )
        if device == "mps":
            model.to("mps")
    except Exception as e:
        print(f"‚ö†Ô∏è  Falling back to SDPA attention due to error: {e}")
        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch_dtype,
            attn_implementation="sdpa",
            device_map=device if device != "mps" else None
        )
        if device == "mps":
            model.to("mps")
            
    model.eval()
    model.set_ddpm_inference_steps(num_steps=5) # Default from demo

    voice_mapper = VoiceMapper(VOICES_DIR)

except Exception as e:
    print(f"‚ùå Error loading VibeVoice model: {e}")
    sys.exit(1)




# Generate filename dynamically
TEXT = clean_text(TEXT)
first_line = TEXT.strip().split('\n')[0]
date_match = re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", TEXT)
if date_match:
    m, d, y = date_match.groups()
    date_str = f"{y}-{int(m):02d}-{int(d):02d}"
else:
    date_match = re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", TEXT)
    if date_match:
        y, m, d = date_match.groups()
        date_str = f"{y}-{int(m):02d}-{int(d):02d}"
    else:
        date_str = datetime.today().strftime("%Y-%m-%d")

# 2. Extract Verse
verse_ref = filename_parser.extract_verse_from_text(TEXT)

if verse_ref:
    extracted_prefix = CLI_PREFIX if CLI_PREFIX else filename_parser.extract_filename_prefix(TEXT)
    filename = filename_parser.generate_filename(verse_ref, date_str, extracted_prefix, base_name="Prayer").replace(".mp3", "_vibe.mp3")
else:
    filename = f"SOH_Sound_of_Home_Prayer_{date_str}_vibe.mp3"

OUTPUT_DIR = os.path.join(os.getcwd(), "output")
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
OUTPUT_PATH = os.path.join(OUTPUT_DIR, filename)
print(f"Target Output: {OUTPUT_PATH}")

TEXT = convert_bible_reference(TEXT)
TEXT = convert_dates_in_text(TEXT)
TEXT = clean_text(TEXT)

paragraphs = [p.strip() for p in re.split(r'\n{2,}', TEXT.strip()) if p.strip()]

# VibeVoice Synthesis Function
def speak(text: str, voice_name: str) -> AudioSegment:
    print(f"DEBUG: Text to read: {text[:100]}...")
    print(f"   Synthesizing ({len(text)} chars) with {voice_name}...")
    
    voice_path = voice_mapper.get_voice_path(voice_name)
    if not voice_path:
        print(f"   ‚ùå Voice '{voice_name}' not found, falling back to default.")
        voice_path = voice_mapper.get_voice_path("Mike") # Fallback

    # Load voice prompt
    all_prefilled = torch.load(voice_path, map_location=device, weights_only=False)

    # Process Inputs
    inputs = processor.process_input_with_cached_prompt(
        text=text,
        cached_prompt=all_prefilled,
        padding=True,
        return_tensors="pt",
        return_attention_mask=True,
    )
    
    # Move to device
    for k, v in inputs.items():
        if torch.is_tensor(v):
            inputs[k] = v.to(device)
            
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=None,
            cfg_scale=1.5,
            tokenizer=processor.tokenizer,
            generation_config={'do_sample': False},
            verbose=False,
            all_prefilled_outputs=all_prefilled
        )
        
    if outputs.speech_outputs and outputs.speech_outputs[0] is not None:
        audio_np = outputs.speech_outputs[0].cpu().numpy()
        # VibeVoice output is float -1..1, usually 24kHz (check demo, said 24000)
        audio_int16 = (audio_np * 32767).astype(np.int16)
        
        return AudioSegment(
            audio_int16.tobytes(),
            frame_rate=24000,
            sample_width=2,
            channels=1
        )
    else:
        print("   ‚ùå No audio generated.")
        print("\nOptions:")
        print("  (Note: You can add 'FilenamePrefix: <Prefix>' in the input TEXT to customize output filename)")
        return AudioSegment.silent(duration=500)

final_mix = AudioSegment.empty()
silence = AudioSegment.silent(duration=800, frame_rate=24000)

# Multi-Voice Rotation
voices = ["Mike", "Emma", "Frank", "Grace", "Carter"]
print(f"Available Voices being used: {voices}")

for i, para in enumerate(paragraphs):
    current_voice = voices[i % len(voices)]
    print(f"  > Para {i+1} - {current_voice}")
    
    try:
        segment = speak(para, current_voice)
        final_mix += segment
        if i < len(paragraphs) - 1:
            final_mix += silence
    except Exception as e:
        print(f"‚ùå Error generating para {i}: {e}")

final_mix = final_mix.set_frame_rate(24000)

# Add Background Music (Optional)
if ENABLE_BGM:
    print("üéµ Mixing Background Music...")
    final_mix = audio_mixer.mix_bgm(final_mix, specific_filename=BGM_FILE)
else:
    print("üéµ Background Music: Disabled (ENABLE_BGM=False)")

# Metadata extraction
PRODUCER = "VI AI Foundation"
TITLE = TEXT.strip().split('\n')[0]

final_mix.export(OUTPUT_PATH, format="mp3", bitrate="192k", tags={'title': TITLE, 'artist': PRODUCER})
print(f"‚úÖ Saved: {OUTPUT_PATH}")
