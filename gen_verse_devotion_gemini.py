# gen_verse_devotion_gemini.py
# Google Gemini TTS ‚Äì Multi-voice, works with Google Cloud credentials

import io
import os
import re
from google.cloud import texttospeech
from pydub import AudioSegment

from bible_parser import convert_bible_reference
from date_parser import convert_dates_in_text
from text_cleaner import remove_space_before_god
import filename_parser
import re
from datetime import datetime

# Ensure Google Cloud credentials are set
# os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/path/to/key.json" 
# Verify credentials exist
if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
    print("‚ö†Ô∏è WARNING: GOOGLE_APPLICATION_CREDENTIALS not set. Ensure you have authenticated via gcloud or set the env var.")



TEXT = """
‰ΩèÂú®Á•ûÁöÑÁà±ÈáåÈù¢ (Á∫¶Áø∞‰∏Ä‰π¶ 4:16) 12/13/2025

ÂèäËá≥Êó∂ÂÄôÊª°Ë∂≥Ôºå„ÄÄÁ•ûÂ∞±Â∑ÆÈÅ£‰ªñÁöÑÂÑøÂ≠êÔºå‰∏∫Â•≥Â≠êÊâÄÁîüÔºå‰∏îÁîüÂú®ÂæãÊ≥ï‰ª•‰∏ãÔºåË¶ÅÊääÂæãÊ≥ï‰ª•‰∏ãÁöÑ‰∫∫ËµéÂá∫Êù•ÔºåÂè´Êàë‰ª¨ÂæóÁùÄÂÑøÂ≠êÁöÑÂêçÂàÜ„ÄÇ‰Ω†‰ª¨Êó¢‰∏∫ÂÑøÂ≠êÔºå„ÄÄÁ•ûÂ∞±Â∑Æ‰ªñÂÑøÂ≠êÁöÑÁÅµËøõÂÖ•‰Ω†‰ª¨ÔºàÂéüÊñáÊòØÊàë‰ª¨ÔºâÁöÑÂøÉÔºåÂëºÂè´Ôºö‚ÄúÈòøÁà∏ÔºÅÁà∂ÔºÅ‚ÄùÂèØËßÅÔºå‰ªéÊ≠§‰ª•ÂêéÔºå‰Ω†‰∏çÊòØÂ•¥‰ªÜÔºå‰πÉÊòØÂÑøÂ≠ê‰∫ÜÔºõÊó¢ÊòØÂÑøÂ≠êÔºåÂ∞±Èù†ÁùÄ„ÄÄÁ•û‰∏∫ÂêéÂó£„ÄÇ
(Âä†ÊãâÂ§™‰π¶ 4:4-7 ÂíåÂêàÊú¨)
‰ΩÜÂà∞‰∫ÜÊó∂Êú∫ÊàêÁÜüÔºå„ÄÄÁ•ûÂ∞±Â∑ÆÈÅ£‰ªñÁöÑÂÑøÂ≠êÔºåÁî±Â•≥‰∫∫ÊâÄÁîüÔºåËÄå‰∏îÁîüÂú®ÂæãÊ≥ï‰πã‰∏ãÔºåË¶ÅÊääÂæãÊ≥ï‰πã‰∏ãÁöÑ‰∫∫ÊïëËµéÂá∫Êù•ÔºåÂ•ΩËÆ©Êàë‰ª¨ÂæóÁùÄÂó£Â≠êÁöÑÂêçÂàÜ„ÄÇ
(Âä†ÊãâÂ§™‰π¶ 4:4-5 Êñ∞ËØëÊú¨)
ÊâÄ‰ª•Ôºå‰Ω†‰ª¨Ë¶ÅËá™ÂçëÔºåÊúçÂú®„ÄÄÁ•ûÂ§ßËÉΩÁöÑÊâã‰∏ãÔºåÂà∞‰∫ÜÊó∂ÂÄôÔºå‰ªñÂøÖÂè´‰Ω†‰ª¨ÂçáÈ´ò„ÄÇ‰Ω†‰ª¨Ë¶ÅÂ∞Ü‰∏ÄÂàáÁöÑÂøßËôëÂç∏Áªô„ÄÄÁ•ûÔºåÂõ†‰∏∫‰ªñÈ°æÂøµ‰Ω†‰ª¨„ÄÇ
(ÂΩºÂæóÂâç‰π¶ 5:6-7)


„ÄÄÁ•ûÁà±Êàë‰ª¨ÁöÑÂøÉÔºåÊàë‰ª¨‰πüÁü•ÈÅì‰πü‰ø°„ÄÇ
„ÄÄÁ•ûÂ∞±ÊòØÁà±Ôºõ‰ΩèÂú®Áà±ÈáåÈù¢ÁöÑÔºåÂ∞±ÊòØ‰ΩèÂú®„ÄÄÁ•ûÈáåÈù¢Ôºå„ÄÄÁ•û‰πü‰ΩèÂú®‰ªñÈáåÈù¢„ÄÇ
(Á∫¶Áø∞‰∏Ä‰π¶ 4:16 ÂíåÂêàÊú¨)
„ÄÄÁ•ûÂØπÊàë‰ª¨ÁöÑÁà±ÔºåÊàë‰ª¨Â∑≤ÁªèÊòéÁôΩ‰∫ÜÔºåËÄå‰∏îÁõ∏‰ø°‰∫Ü„ÄÇ
„ÄÄÁ•ûÂ∞±ÊòØÁà±Ôºõ‰ΩèÂú®Áà±ÈáåÈù¢ÁöÑÔºåÂ∞±‰ΩèÂú®„ÄÄÁ•ûÈáåÈù¢Ôºå„ÄÄÁ•û‰πü‰ΩèÂú®‰ªñÈáåÈù¢„ÄÇ
(Á∫¶Áø∞Â£π‰π¶ 4:16 Êñ∞ËØëÊú¨)
ËøôÊ†∑ÔºåÊàë‰ª¨Â∑≤ÁªèËÆ§ËØÜÂπ∂Áõ∏‰ø°‰∫ÜÁ•ûÂØπÊàë‰ª¨ÊâÄÊÄÄÁöÑÁà±„ÄÇÁ•ûÂ∞±ÊòØÁà±ÔºõÈÇ£‰ΩèÂú®Áà±ÈáåÈù¢ÁöÑÔºåÂ∞±‰ΩèÂú®Á•ûÈáåÈù¢ÔºåÁ•û‰πü‰ΩèÂú®‰ªñÈáåÈù¢„ÄÇ
(Á∫¶Áø∞‰∏Ä‰π¶ 4:16 Ê†áÂáÜËØëÊú¨)

‰ΩèÂú®Á•ûÁöÑÁà±ÈáåÈù¢

‰Ω†ÊõæÈÅáÂà∞ËøáÁâπÂà´ÂñÑËâØÂíå‰ΩìË¥¥ÁöÑ‰∫∫ÂêóÔºüÂ•ΩÊúãÂèãÂ∞±ÊòØËøôÊ†∑‚Äî‚ÄîÂπ≥ÊòìËøë‰∫∫„ÄÅÁÉ≠ÂøÉÊÉ≥Áü•ÈÅì‰Ω†ËøáÂæóÊÄé‰πàÊ†∑„ÄÅÂøÉÊó†ÊóÅÈ™õÂú∞ÂÄæÂê¨‰Ω†„ÄÇÂ•ΩÊúãÂèã‰ºö‰ΩøÊàë‰ª¨ËÆ∞Ëµ∑Êàë‰ª¨ÊòØË∞Å„ÄÇ‰ªñ‰ª¨ÊÄÄÁùÄ‰ªÅÊÖà‰∏éÁà±Êù•ÂÄæÂê¨‰Ω†ÊâÄËØ¥ÁöÑ‰∏ÄÂàáÔºåÊó†ËÆ∫ÊòØÂñúËøòÊòØÂøß„ÄÇ

Á•ûÂ∞±ÊòØËøôÊ†∑ÁöÑÊúãÂèã„ÄÇ‰ªñÂÄæÂê¨Ôºõ‰ªñÊÑüÂêåË∫´Âèó„ÄÇ‰ªñÈùûÂ∏∏ÂÖ≥ÂøÉ‰Ω†ÔºåÂπ∂‰∏î‰ªÅÊÖàÂú∞ÂõûÂ∫î‰Ω†„ÄÇ‰∫ãÂÆû‰∏äÔºåÁ•ûÊâÄÂÅöÁöÑ‰∏ç‰ªÖ‰ªÖÊòØÂ±ïÁ§∫Áà±‚Äî‚Äî‰ªñÂ∞±ÊòØÁà±„ÄÇ‰ªñ‰∏çÂèØËÉΩ‰∏çÁà±ÔºåÂõ†‰∏∫Áà±Â∞±ÊòØ‰ªñÁöÑÊú¨Ë¥®„ÄÇ‰ªñÁöÑÁà±ÊòØÁ∫ØÊ¥ÅÁöÑÔºõ‰∏çËá™ÁßÅ„ÄÅ‰∏çËÑ±ËäÇ„ÄÅ‰∏çËã¶Ê∂©„ÄÅ‰∏çÊÄ®ÊÅ®Ôºå‰πü‰∏çÊ∂àÊûÅ„ÄÇÊàë‰ª¨ÂèØ‰ª•‰ø°Èù†ËøôÁßçÁà±ÔºõÊàë‰ª¨ÂèØ‰ª•‰ø°Èù†Á•û„ÄÇ

Âú®Á∫¶Áø∞‰∏Ä‰π¶ 4:16 ‰∏≠ÔºåÊàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∏™ÁæéÂ•ΩÁöÑÊèêÈÜíÔºåËÆ©Êàë‰ª¨Áü•ÈÅì‰∏éÁ•ûÂêåÂú®ÁöÑÁîüÊ¥ªÊòØ‰ªÄ‰πàÊ†∑ÁöÑÔºö‚ÄúÁ•ûÁà±Êàë‰ª¨ÁöÑÂøÉÔºåÊàë‰ª¨‰πüÁü•ÈÅì‰πü‰ø°„ÄÇÁ•ûÂ∞±ÊòØÁà±Ôºõ‰ΩèÂú®Áà±ÈáåÈù¢ÁöÑÔºåÂ∞±ÊòØ‰ΩèÂú®Á•ûÈáåÈù¢ÔºåÁ•û‰πü‰ΩèÂú®‰ªñÈáåÈù¢„ÄÇ‚Äù

ÂíåÂ•ΩÊúãÂèãÂÖ±Â∫¶Êó∂ÂÖâÂêéÔºå‰Ω†ÊÑüËßâÂ¶Ç‰ΩïÔºü‰πüËÆ∏‰Ω†‰ºöËßâÂæóÂøÉÊÉÖÂÆâÈÄ∏„ÄÅÊ≠•Â±•ËΩªÂø´ÔºåÊàñËÄÖ‰Ω†ÂèëÁé∞Ëá™Â∑±Êúâ‰∫ÜÂãáÊ∞îÁªßÁª≠ÂâçË°å„ÄÇ‰Ω†ÁîöËá≥ÂèØËÉΩÂèëÁé∞Ëá™Â∑±Âõ†‰∏∫ÊÑüÂèóÂà∞Ë¢´Áà±ËÄåÊõ¥Âä†Áà±Âà´‰∫∫„ÄÇËøôÂ∞±ÊòØ‰ΩèÂú®Á•ûÁöÑÁà±‰∏≠‰∫ßÁîüÁöÑËøûÈîÅÂèçÂ∫î„ÄÇÂΩì‰Ω†Áü•ÈÅìÂπ∂‰ΩìÈ™åÂà∞Á•ûÂ§ö‰πàÁà±‰Ω†Êó∂Ôºå‰Ω†Â∞±‰ºöÊÉÖ‰∏çËá™Á¶ÅÂú∞Áà±Âà´‰∫∫„ÄÇ

ËøôÂ∞±ÊòØÊàë‰ª¨Ë¢´ÈÇÄËØ∑ÂéªËøáÁöÑÁîüÊ¥ª„ÄÇ‰∏ÄÁßç‰∫ÜËß£Âπ∂‰æùËµñÁ•ûÂØπÊàë‰ª¨ÁöÑÁà±ÔºåÁÑ∂ÂêéÂõ†Ê≠§ËÄåÁà±‰ªñ‰∫∫ÁöÑÁîüÊ¥ª„ÄÇ‰ªäÂ§©Ôºå‰Ω†Â∞ÜÂ¶Ç‰ΩïÊâæÂà∞Á•ûÂØπ‰Ω†ÁöÑÁà±ÔºüÂú®‰Ω†ËÆ§ËØÜÂπ∂‰ΩìÈ™åÂà∞Á•ûÂØπ‰Ω†ÁöÑÁà±‰πãÂêéÔºå‰∏ÄÂàáÈÉΩ‰ºöÊîπÂèò„ÄÇ

Á•∑ÂëäÔºö
Á•ûÂïäÔºå‰Ω†ÁöÑÁà±ÂØπÊàëÊù•ËØ¥Â§™Â•áÂ¶ô‰∫Ü„ÄÇÊàëÈúÄË¶ÅÊõ¥Âä†ÁêÜËß£‰Ω†ÁöÑÁà±„ÄÇÊàëÊÉ≥ÊÑüÂèóË¢´Áà±ÂíåË¢´ÈáçËßÜÔºõÊàëÊÉ≥ÁªèÂéÜ‰Ω†ÂØπÊàë‰ΩïÁ≠âÁöÑÁú∑È°æ„ÄÇËØ∑‰Ω†‰ªäÂ§©Â∞±Â∏ÆÂä©ÊàëÊõ¥Ê∑±ÂÖ•ËÆ§ËØÜ‰Ω†ÁöÑÁà±„ÄÇÂ•âËÄ∂Á®£ÁöÑÂêçÔºåÈòø‰ª¨„ÄÇ
"""

# Generate filename dynamically
# 1. Extract Date
first_line = TEXT.strip().split('\n')[0]
date_match = re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", first_line)
if date_match:
    m, d, y = date_match.groups()
    date_str = f"{y}-{int(m):02d}-{int(d):02d}"
else:
    # Try YYYY-MM-DD
    date_match = re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", first_line)
    if date_match:
        y, m, d = date_match.groups()
        date_str = f"{y}-{int(m):02d}-{int(d):02d}"
    else:
        date_str = datetime.today().strftime("%Y-%m-%d")

# 2. Extract Verse (First parenthesis content)
verse_match = re.search(r"\((.*?)\)", TEXT)
verse_ref = verse_match.group(1).strip() if verse_match else "Unknown-Verse"

filename = filename_parser.generate_filename(verse_ref, date_str)
OUTPUT_DIR = os.getcwd()
OUTPUT_PATH = os.path.join(OUTPUT_DIR, filename)
print(f"Target Output: {OUTPUT_PATH}")

TEXT = convert_bible_reference(TEXT)
TEXT = convert_dates_in_text(TEXT)
TEXT = remove_space_before_god(TEXT)

paragraphs = [p.strip() for p in TEXT.strip().split("\n\n") if p.strip()]

# Supported Gemini/Google Voices
# Using a mix of male/female voices for variety
voices = ["Charon", "Kore", "Fenrir", "Aoede", "Puck"]

# Voice configuration
LANGUAGE_CODE = "cmn-CN"
MODEL_NAME = "gemini-2.5-pro-tts" # Or appropriate model

# Global client cache
_TTS_CLIENT = None

def get_tts_client():
    global _TTS_CLIENT
    if _TTS_CLIENT:
        return _TTS_CLIENT

    try:
        # Try default credentials first
        _TTS_CLIENT = texttospeech.TextToSpeechClient()
        return _TTS_CLIENT
    except Exception as e:
        print(f"‚ö†Ô∏è Default auth failed: {e}")
        print("üîÑ Attempting to use gcloud access token...")
        
        try:
            import subprocess
            import google.oauth2.credentials
            from google.api_core.client_options import ClientOptions
            
            # Get token via gcloud
            result = subprocess.run(
                ["zsh", "-l", "-c", "gcloud auth print-access-token"],
                capture_output=True,
                text=True,
                check=True
            )
            token = result.stdout.strip()
            
            # Get project ID via gcloud for quota
            project_result = subprocess.run(
                ["zsh", "-l", "-c", "gcloud config get-value project"],
                capture_output=True,
                text=True,
                check=True
            )
            project_id = project_result.stdout.strip()
            
            if not token:
                raise ValueError("Empty token received from gcloud")
                
            creds = google.oauth2.credentials.Credentials(token=token)
            
            # Set quota project to fix 403 error
            client_options = ClientOptions(quota_project_id=project_id) if project_id else None
            
            _TTS_CLIENT = texttospeech.TextToSpeechClient(credentials=creds, client_options=client_options)
            print(f"‚úÖ Successfully authenticated using gcloud access token (Project: {project_id}).")
            return _TTS_CLIENT
            
        except Exception as token_error:
            print(f"‚ùå Failed to get gcloud token/project: {token_error}")
            raise e


def chunk_text(text: str, max_len: int = 400) -> list[str]:
    """Split text if too long (though Google limit is high, good practice)."""
    if len(text) <= max_len:
        return [text]
    chunks = []
    current_chunk = ""
    parts = re.split(r'([„ÄÇÔºÅÔºüÔºõ!.?\n]+)', text)
    for part in parts:
        if len(current_chunk) + len(part) < max_len:
            current_chunk += part
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = part
    if current_chunk:
        chunks.append(current_chunk)
    return chunks

def speak(text: str, voice: str, style_prompt: str = None) -> AudioSegment:
    client = get_tts_client()
    synthesis_input = texttospeech.SynthesisInput(text=text)
    
    # Selecting voice
    voice_params = texttospeech.VoiceSelectionParams(
        language_code=LANGUAGE_CODE,
        name=voice,
        # Enable model name for Gemini voices
        model_name=MODEL_NAME 
    )
    
    # Audio config
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )
    
    # Request
    request = texttospeech.SynthesizeSpeechRequest(
        input=synthesis_input,
        voice=voice_params,
        audio_config=audio_config
    )
    
    # If style prompt is provided... (same logic as before)
    if style_prompt:
         try:
             synthesis_input = texttospeech.SynthesisInput(text=text, prompt=style_prompt)
             # Re-assign input to request
             request.input = synthesis_input
         except TypeError:
             print(f"‚ö†Ô∏è Warning: 'prompt' not supported in SynthesisInput for this client version. Ignoring prompt: {style_prompt}")

    try:
        response = client.synthesize_speech(request=request)
        return AudioSegment.from_mp3(io.BytesIO(response.audio_content))
    except Exception as e:
        # Check for safety filter error (400 InvalidArgument)
        if "sensitive or harmful content" in str(e) or "400" in str(e):
            print(f"‚ö†Ô∏è Safety filter triggered for voice {voice}. Retrying strategies...")
            
            # Strategy 1: Remove style prompt if present
            if style_prompt:
                print("   ‚Ü≥ Strategy 1: Removing style prompt...")
                try:
                    # Clear prompt from input
                    request.input = texttospeech.SynthesisInput(text=text)
                    response = client.synthesize_speech(request=request)
                    print("   ‚úÖ Strategy 1 success.")
                    return AudioSegment.from_mp3(io.BytesIO(response.audio_content))
                except Exception as e2:
                    print(f"   ‚ùå Strategy 1 failed: {e2}")

            # Strategy 2: Fallback to standard Wavenet voice (High quality, widely available)
            print("   ‚Ü≥ Strategy 2: Fallback to standard Wavenet voice...")
            try:
                # Use Wavenet voices for Chinese (Neural2 is not fully supported for cmn-CN yet)
                # cmn-CN-Wavenet-A (Female), cmn-CN-Wavenet-B (Male), cmn-CN-Wavenet-C (Male), cmn-CN-Wavenet-D (Female)
                
                # Simple mapping
                fallback_voice = "cmn-CN-Wavenet-C" # Male default (replaces Charon/Fenrir/Puck)
                if voice in ["Kore", "Aoede"]:
                    fallback_voice = "cmn-CN-Wavenet-A" # Female default (replaces Kore/Aoede)
                
                fallback_params = texttospeech.VoiceSelectionParams(
                    language_code=LANGUAGE_CODE,
                    name=fallback_voice
                    # No model_name needed for standard voices
                )
                
                # Reset request with new voice and simple input (no prompt)
                request.voice = fallback_params
                request.input = texttospeech.SynthesisInput(text=text)
                
                response = client.synthesize_speech(request=request)
                print(f"   ‚úÖ Strategy 2 success (Used {fallback_voice}).")
                return AudioSegment.from_mp3(io.BytesIO(response.audio_content))
                
            except Exception as e3:
                print(f"   ‚ùå Strategy 2 failed: {e3}")
                raise e # Give up if even standard voice fails
        
        # Re-raise other errors
        raise e

# Group paragraphs into 5 logical sections
# 1. Intro (Title/Date)
# 2. Scripture 1
# 3. Scripture 2
# 4. Main Body (Middle paragraphs)
# 5. Prayer (Last paragraph)

if len(paragraphs) < 5:
    # Fallback
    logical_sections = [[p] for p in paragraphs]
else:
    logical_sections = [
        [paragraphs[0]],              # Intro
        [paragraphs[1]],              # Scripture 1
        [paragraphs[2]],              # Scripture 2
        paragraphs[3:-1],             # Main Body
        [paragraphs[-1]]              # Prayer
    ]

# Ensure we don't exceed available voices
num_sections = len(logical_sections)
section_titles = ["Intro", "Scripture 1", "Scripture 2", "Main Body", "Prayer"]

# Define Style Prompts for each section
# These prompts help guide the "Gemini" model to adopt the correct tone
section_styles = [
    "Speak with a clear, professional, announcer-like tone for a title introduction.", # Intro
    "Speak with a solemn, reverent, and slow pace, suitable for reading Holy Scripture.", # Scripture 1
    "Speak with a solemn, reverent, and slow pace, suitable for reading Holy Scripture.", # Scripture 2
    "Speak with a warm, engaging, and personal storytelling tone, like a friend sharing wisdom.", # Main Body
    "Speak with a humble, quiet, and earnest tone, suitable for prayer.", # Prayer
]

print(f"Processing {num_sections} logical sections...")

final_segments = []
global_p_index = 0

for i, section_paras in enumerate(logical_sections):
    title = section_titles[i] if i < len(section_titles) else f"Section {i+1}"
    style_prompt = section_styles[i] if i < len(section_styles) else None
    
    print(f"--- Section {i+1}: {title} ---")
    if style_prompt:
        print(f"  [Style Prompt]: {style_prompt}")
    
    section_audio = AudioSegment.empty()
    # Assuming silence same as 'silence' defined later, but we need it here.
    # We'll use 24000 as default or detect from first segment if available.
    silence_between_paras = AudioSegment.silent(duration=700, frame_rate=24000)

    for j, para in enumerate(section_paras):
        voice = voices[global_p_index % len(voices)]
        print(f"  > Part {i+1}.{j+1} - {voice} ({len(para)} chars)")
        global_p_index += 1
        
        current_segment = AudioSegment.empty()
        # Check length
        if len(para) > 1000:
            chunks = chunk_text(para, 1000)
            print(f"    Split into {len(chunks)} chunks due to length.")
            for chunk in chunks:
                if chunk.strip():
                    current_segment += speak(chunk, voice, style_prompt)
        else:
            current_segment = speak(para, voice, style_prompt)
            
        section_audio += current_segment
        
        if j < len(section_paras) - 1:
            section_audio += silence_between_paras
            
    final_segments.append(section_audio)

final = AudioSegment.empty()

# Determine sample rate from first segment to ensure consistency
# Determine sample rate from first segment to ensure consistency
frame_rate = final_segments[0].frame_rate if final_segments else 24000
silence = AudioSegment.silent(duration=1000, frame_rate=frame_rate)

for i, seg in enumerate(final_segments):
    final += seg
    if i < len(final_segments) - 1:
        final += silence

final.export(OUTPUT_PATH, format="mp3", bitrate="192k")
print(f"Success! Saved ‚Üí {OUTPUT_PATH}")
